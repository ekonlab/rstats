---
title: "Transportation Analysis | Public Bikes Sharing Systems"
author: "Alberto González Paje"
date: "December 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction to this chapter

This chapter covers data analysis of public bike sharing systems around the world. It's the first of a few chapters covering different transport means, such as airplanes, buses, cars and others. The idea of this project is to apply data analysis techniques to real world data sets.
In this first chapter I will show a full data analysis cycle, starting in gathering data and ending by creating a prediction model of public bike sharing system capacity in London city.

This document is built using markdown syntax and then generated HTML code for publishing the whole on the Internet. The flow is basically splitted in three parts, gathering, storing and modeling data.

# Get Public Bicicles Sharing Systems

The following code is coming from get_citybik.R
In order to gather data, I will be using CityBikes API www.citybik.es where you can access in real time to data of dozens of public bike sharing systems around the world.

## 1.- Get list of all systems involved

First we need to set our current working directory. Beware of changing this working directory if you happen to be working with these data by your own. 

```{r,message=FALSE}

# 1.-GOAL: Get list of bikes systems and complete with full URL 

# Load jsonlite package
library(jsonlite)

# Set working directory (Remember to change this directory on demand)
#setwd("/home/albertogonzalez/Desktop/TDA/chapters/bikes")
```

Get a list of all networks using the network endpoint. Basically I want to know which bike systems are available using this endpoint.

```{r,message=FALSE}
# Get all networks involved
network_endpoint = "http://api.citybik.es/v2/networks"
network_endpoint_df <- jsonlite::fromJSON(network_endpoint, simplifyDataFrame = TRUE)

```

Get valid data from the list of all networks and incorporate a column with the suffix URL.

```{r,message=FALSE}
# Refine endpoint output
company_names = network_endpoint_df$networks
company_ids = company_names$id
company_data = company_names$location
company_url = company_names$href

# Build a suffix to endpoint
suffix = "http://api.citybik.es"
company_url = paste(suffix,company_names$href,sep="")
company_df = cbind(company_ids,company_data,company_url)
```

This is the resulting table:

```{r }
head(company_df)

```

Export a CSV file with the previous dataframe just in case we need this information at any time.

```{r}
# Export list of companies
write.csv(company_df,file = "list_of_bikes_system_worldwide.csv",row.names = FALSE)
```


## 2.- Get data from all systems involved

Now I am going to create a function to get data from all our systems.

```{r}
head(company_df)
```

I will start with one system and then apply the same function to a set of cities, although you could do it for the >400 cities included in this magnificent API.

```{r,message=FALSE}
# 2.- For each system, get data (build DB and cron job)

# Build a function to read, parse and store endpoint data
bike_input = "http://api.citybik.es/v2/networks/wroclawski-rower-miejski"
# Read one system
bike_input_df = jsonlite::fromJSON(bike_input, simplifyDataFrame = TRUE)

bike_input_df_2 = bike_input_df$network
bike_input_df_3 = bike_input_df_2$stations

bike_df = bike_input_df_3[c(1,3:8)]
total_slots = bike_input_df_3$extra$slots
bike_df$total_slots = total_slots

```

The resulting table of capacity data in a given city:

```{r}
head(bike_df)
```

You can see some of the cities included in the API.

```{r}
# Get id + url
input_array = company_df[c(1,6)]
```

```{r}
head(input_array)
```

I will build two arrays with key information, the city id and its API url, which are needed to build the final query.

```{r }
array_id = input_array$company_ids
array_id = as.data.frame(array_id)
array_url = input_array$company_url
array_url = as.data.frame(array_url)
```

```{r }
head(array_id)
head(array_url)
```


As previously shown, let's create a function to gather data from one city, and then apply that same function to more than one city.

```{r}
# Create the function for one system
foo = function(url,id){
  input = as.character(url)
  read_input = jsonlite::fromJSON(input, simplifyDataFrame = TRUE)
  input_df_2 = read_input$network
  input_df_3 = input_df_2$stations
  input_df = input_df_3[c(1,3:8)]
  input_df$company_id = as.character(id)
  return (input_df)
}

```

Now it's time to test the function in one city and then to a small amouny of cities and ultimately, if you wish, to the whole array of cities.

```{r}
# Test fuction for one:
aa = array_url[2, ] 
aa
a = foo(aa,"wroclawski-rower-miejski")
head(a)
```


The next rows are meant to apply the foo (get data from one system) function to the 452 systems available in Citybikes.

```{r}
# Apply function to all the bike systems

# u = array_url[1:452,]
# i = array_id[1:452,]
# 
# foo_all <- lapply(u,foo,i)

```

So the above chunk shows the techniques to play with the API. Now I will start to get real time data of eight european cities each hour and store that data in a SQLite Database.


## 3.- Get data from selected systems and store them in SQLite DB

Using the function created before, I test first one city.

```{r}
# Test one city.

u = "http://api.citybik.es/v2/networks/dublinbikes"
i = "dublin"

city_ind = foo(u,i)
head(city_ind)
```



Then I create (just the first time) the SQLite database nd store the results for future analysis.

```{r}
# Create a DB (sqlite)
library(dplyr)

# Create DB & Table
# my_db <- src_sqlite( "bikes.sqlite3", create = TRUE)   # crear DB          
# copy_to( my_db, city_ind, "bikes_records_2", temporary = FALSE)   # crear tabla        

```

Set up a cron job to get and store data for the selected list of systems with the following code (see store_citybik.R)

In this following code chunk you will see the case of one city (Dublin) but if you go to store_citybik.R you will see how I got data from eight different cities in the cron job.

```{r }
#  libraries
library(RSQLite)
#setwd("/home/albertogonzalez/Desktop/bestiario/Quadrigram/2016/monografias/bikes")

# Open DB connection
con <- dbConnect(SQLite(),dbname="bikes.sqlite3")
my_db <- src_sqlite( "bikes.sqlite3", create = FALSE)


# Function to get data
foo = function(url,id){
  input = as.character(url)
  read_input = jsonlite::fromJSON(input, simplifyDataFrame = TRUE)
  input_df_2 = read_input$network
  input_df_3 = input_df_2$stations
  input_df = input_df_3[c(1,3:8)]
  input_df$company_id = as.character(id)
  return (input_df)
}


# DUBLIN
u = "http://api.citybik.es/v2/networks/dublinbikes"
i = "dublin"

city_ind = foo(u,i)
dim(city_ind)
head(city_ind)
```



Once you got data, you need to insert it into the database.

```{r}
# Append rows to DB:
db_insert_into( con = my_db$con, table = "bikes_records_2", values = city_ind) # append rows

```


## 4.- Read DB and Model

The final part of this process is coming from model_bikes.R
I have been gathering data from those eight cities for a couple of months, therefore I have a rich dataset which in reality is a bunch of datasets that can be used to apply data analysis to any of those european cities.


### Read DB

```{r, message=FALSE}
library(jsonlite)
library(RSQLite)
library(lubridate)
library(tidyr)
library(ggmap)
library(ggthemes)
library(dplyr)

```

The first thing to do is to connect to the database where I stored data and get it all.

```{r}
# Open connection
con <- dbConnect(SQLite(),dbname="bikes.sqlite3")
my_db <- src_sqlite( "bikes.sqlite3", create = FALSE)

# Get all data
all_data = dbGetQuery(con,"SELECT * FROM bikes_records_2")
str(all_data)
head(all_data)
```


Let's apply some time transformations. 

```{r}
# Insert dates columns
#all_data$timestamp = as.Date(all_data$timestamp)
all_data$time_2 = ymd_hms(all_data$timestamp)
all_data$week_day = wday(all_data$time_2,label = TRUE)
all_data$hour = hour(all_data$time_2)
all_data$shift = cut(all_data$hour,c(0,7,15,23),c("night","morning","afternoon"))

```



We can define a capacity status in terms of how many bikes are being used in a given station. We define a station as overflow when there is few empty slots to park. As far as I have seen is other similar analysis, thresholds to score a station as overflow, balanced or shortage are subjective. In this analysis, stations with empty slots from 0% to 25% are scored as overflow. Stations with empty slots from 75% to 100% are scored as shortage and everything in between is balanced.

```{r}
# Total slots and shifts calculations
all_data$total_slots = all_data$empty_slots + all_data$free_bikes

all_data$empty_status = (all_data$empty_slots / all_data$total_slots) * 100
all_data$empty_status_label = cut(all_data$empty_status,c(0,25,75,100),c("overflow","balanced","shortage"))

all_data$empty_status_label[all_data$empty_status==0] <- "overflow"
head(all_data)
```


Now it's time to focus on a single city, in this case, London.

```{r}
# One City
ci = "london" # line 38 in model bikes.R

ci_df = all_data %>%
  filter(company_id == ci)

dim(ci_df)
ci_df = na.omit(ci_df)
dim(ci_df)

head(ci_df)

```


Mean capacity by weekday, hour and station.

```{r}
## Group by weekday, hour and station - mean of used_capacity
ci_df_2 = ci_df %>%
  group_by(week_day,hour,name) %>%
  summarise(avg_used_capacity = mean(empty_status, na.rm=TRUE)) %>%
  arrange(desc(avg_used_capacity))


ci_df_ref = as.data.frame(ci_df_2)

head(ci_df_ref)

```



We can apply a rank technique to get X number of stations:

```{r}
# Top and bottom stations
top_stations_weekday = ci_df_ref[c(1:250),]
bottom_stations_weekday = tail(ci_df_ref,250)
```



Although it's simple to get that top and bottom used stations grouping just by station:

```{r}
## Group by station - mean de used_capacity
ci_df_3 = ci_df %>%
  group_by(name) %>%
  summarise(avg_used_capacity = mean(empty_status, na.rm=TRUE)) %>%
  arrange(desc(avg_used_capacity))

ci_df_3_ref = as.data.frame(ci_df_3)
head(ci_df_3_ref)
```

See top and bottom used stations:

```{r }
# Top and Bottom stations
top_stations = ci_df_3_ref[c(1:10),]
top_stations
bottom_stations = tail(ci_df_3_ref,10)
bottom_stations
```

If we want to have a better view, we can geocode those stations.

```{r }
## Geolocate main stations
ci_df_geo = ci_df[,c("name","latitude","longitude")] 
ci_df_geo_uni = unique(ci_df_geo)

top_stations_geo = merge(top_stations,ci_df_geo_uni,by = "name")
bottom_stations_geo = merge(bottom_stations,ci_df_geo_uni,by = "name")

head(top_stations_geo)
head(bottom_stations_geo)
```


Now that we have geocoded top and bottom stations, we can plot them in a map.

```{r, message= FALSE}
m <- get_map("London",zoom=12,maptype="terrain-lines",source="stamen")
```

```{r}
ggmap(m, base_layer = ggplot(aes(x = longitude, y = latitude), data = top_stations_geo))  + geom_point(color="green") + theme_minimal() + ggtitle("Top Stations")

```

```{r}
ggmap(m, base_layer = ggplot(aes(x = longitude, y = latitude), data = bottom_stations_geo))  + geom_point(color="red") + theme_minimal() + ggtitle("Bottom Stations")
```

We can plot the top and bottom stations by week day.

```{r }
top_stations_weekday_geo = merge(top_stations_weekday,ci_df_geo_uni,by = "name")
bottom_stations_weekday_geo = merge(bottom_stations_weekday,ci_df_geo_uni,by = "name")

head(top_stations_weekday_geo)
head(bottom_stations_weekday_geo)
```

```{r,message=FALSE,fig.width=15,fig.height=10}
ggmap(m, base_layer = ggplot(aes(x = longitude, y = latitude), data = top_stations_weekday_geo))  + facet_grid(week_day ~ hour) + geom_point(color="green") + theme_minimal()  + ggtitle("Top Stations by weekday and hour")

```

  
```{r, message=FALSE,fig.width=15,fig.height=10}
ggmap(m, base_layer = ggplot(aes(x = longitude, y = latitude), data = bottom_stations_weekday_geo))  + facet_grid( week_day ~ hour) + geom_point(color="red") + theme_minimal() + ggtitle("Bottom Stations by weekday and hour")

```




```{r}
ci_df_3_geo = merge(ci_df_3,ci_df_geo_uni,by = "name")
colnames(ci_df_3_geo) = c("name","avgcapacity","latitude","longitude")
```


Adding up the empty slots of stations can give us a good proxy of total bikes being used.

```{r}
## Number of total bikes being used
circulando = ci_df %>%
  group_by(hour) %>%
  summarise(bicis_circulando = sum(empty_slots, na.rm=TRUE)) %>%
  arrange(desc(bicis_circulando))

circulando_df = as.data.frame(circulando)
circulando_df$city = ci
head(circulando_df)
```



And then we can plot it.

```{r}
library(scales)
cir_plot = ggplot(circulando_df,aes(as.factor(hour),bicis_circulando)) + geom_point() + theme_minimal() + ggtitle("Total Bikes in movement by hour")

cir_plot + scale_y_continuous(labels = comma)
```

We can also incorporate population data.

```{r}
# Incorporación de las poblaciones en las ciudades
# lista de ciudades
cities = as.data.frame(unique(all_data$company_id))
str(cities)
names(cities) = c("city")
cities_pop = as.data.frame(c(553165,630752,1604555,14025646,1347707,2249975,8673713,1759407))
names(cities_pop) = "population"
cities_df = cbind(cities,cities_pop)
head(cities_df)
```

```{r}
ggplot(cities_df,aes(city,population)) + geom_point() + theme_minimal() + scale_y_continuous(labels = comma)
```

And add a per_mil_capita metric.

```{r}
cities_df_pop = merge(circulando_df,cities_df,by = "city",all.y = FALSE)
cities_df_pop$popu_mil = cities_df_pop$population/1000
cities_df_pop$per_mil_capita = cities_df_pop$bicis_circulando / 
cities_df_pop$popu_mil

head(cities_df_pop)
```



```{r}
ggplot(cities_df_pop,aes(hour,per_mil_capita)) + geom_point() + theme_minimal() + ggtitle("Bikes per mil capita and hour")
```



```{r}
# Average capacity used per hour
ave_per_hour = ci_df %>%
  group_by(hour) %>%
  summarise(ave_cap_h = mean(empty_status,na.rm = TRUE)) %>%
  arrange(desc(ave_cap_h))

ave_per_hour = as.data.frame(ave_per_hour)
cities_df_ave = merge(cities_df_pop,ave_per_hour,by = "hour")
head(cities_df_ave)
```

Let's plot bikes per mil capita vs average used capacity.

```{r}
ggplot(cities_df_ave,aes(ave_cap_h,per_mil_capita)) + geom_point() + geom_smooth(method = lm) + theme_minimal() + ggtitle("Bikes per mil capita and Average used capacity")
```



## Prediction Model

After a succint analysis, it's time to create our prediction model. The goal is to predict a station status (overflow, balanced or shortage) in a given week day and hour.

```{r}
df1 = ci_df[c("empty_slots","free_bikes","latitude","longitude","hour","empty_status","empty_status_label","id","name","shift","week_day")]

df1$empty_status = as.integer(df1$empty_status)
df1 = na.omit(df1)
str(df1)
dim(df1)
```

Now I will normalize data so it can fit in a prediction model.

```{r}
# overflow table
table(df1$empty_status_label)

# normalize function
normalize = function(x){
  return ((x - min(x))/ (max(x) - min(x)))
}

df1_norm = as.data.frame(lapply(df1[1:5], normalize))
head(df1_norm)
```



Basically the model will classify a station depending on the number of free slots, number of available bikes, latitude, longitude and hour of the day. Split the dataset in two sets, 70% for training and 30% for testing. 

```{r}
# Split & model
n = nrow(df1_norm)

train = sample(1:n, size = round(0.7*n), replace=FALSE)

df1_norm_train = df1_norm[train,]
dim(df1_norm_train)
head(df1_norm_train)

df1_norm_test = df1_norm[-train,]
dim(df1_norm_test)
head(df1_norm_test)

df1_train_labels = df1$empty_status_label[train]
df1_test_labels = df1$empty_status_label[-train]
```

Let's train and test the model. I will use knn with k = 21.

```{r}
# Training the model
library(class)
library(gmodels)


df1_test_pred_2 = knn(df1_norm_train,df1_norm_test,cl = df1_train_labels, k = 21)
CrossTable(df1_test_labels,df1_test_pred_2)


df1_test = df1[-train,]
df1_test$label_pred = df1_test_pred_2
```


As you can see in the previous table, the model predicts quite well.

```{r}
table(df1_test$empty_status_label,df1_test$label_pred)
```

```{r}
df1_test$pred = ifelse(df1_test$empty_status_label==df1_test$label_pred,1,0)
```


  
```{r}
df1_test_wrong = df1_test[df1_test$pred == 0, ]
dim(df1_test_wrong)
```



Finally we can create a simple tool to search a given station and see its real and predicted status in a given day and hour.

```{r}
# Generate a vector with the ID/Names list
places = unique(df1_test$id)
length(places)
```


```{r}
places_names = unique(df1_test$name)
head(places_names,20)
```



```{r}
input_place = "001018 - Phillimore Gardens, Kensington"
input_hour = 8
input_weekday = "Mon"

selected_place = filter(df1_test,name == input_place & hour == input_hour & week_day == input_weekday)

table(selected_place$empty_status_label,selected_place$label_pred)

```

